
Running configs.
{'root_path': 'datasets', 'dataset': 'eurosat', 'shots': 16, 'backbone': 'RN50', 'n_prompt_tokens': 4, 'intrinsic_dim': 512, 'budget': 8000, 'bound': 5, 'alpha': 1, 'popsize': 40, 'parallel': False, 'print_every': 50, 'eval_every': 100, 'n': 1, 'batch_size': 256, 'std': 1, 'weight': 0.01, 'train_epoch': 20, 'subsample_classes': 'all', 'seed': 1}

Preparing dataset.
Reading split from /raid0-data/yifan/datasets/eurosat/split_zhou_EuroSAT.json
Creating a 16-shot dataset

Loading visual features and labels from test set.
Initial context: "a photo of a"
Number of context words (tokens): 4
Population Size: 40
Serial Evaluation.
Printing trainable parameters
Printing to verify trainable parameters
name input_param param.shape: torch.Size([40, 512])
/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Steps: 0 	 Acc:48.23456790123457 	 Loss:0
Steps: 1 	 Acc:48.23456790123457 	 Loss:0
Steps: 2 	 Acc:48.23456790123457 	 Loss:0
Steps: 3 	 Acc:48.23456790123457 	 Loss:0
Steps: 4 	 Acc:48.23456790123457 	 Loss:0
Steps: 5 	 Acc:48.23456790123457 	 Loss:0
Steps: 6 	 Acc:48.23456790123457 	 Loss:0
Steps: 7 	 Acc:48.23456790123457 	 Loss:0
Steps: 8 	 Acc:48.23456790123457 	 Loss:0
Steps: 9 	 Acc:48.23456790123457 	 Loss:0
Steps: 10 	 Acc:59.28395061728395 	 Loss:0
Steps: 11 	 Acc:59.28395061728395 	 Loss:0
Steps: 12 	 Acc:59.28395061728395 	 Loss:0
Steps: 13 	 Acc:59.28395061728395 	 Loss:0
Steps: 14 	 Acc:59.28395061728395 	 Loss:0
Steps: 15 	 Acc:59.28395061728395 	 Loss:0
Steps: 16 	 Acc:59.28395061728395 	 Loss:0
Steps: 17 	 Acc:59.28395061728395 	 Loss:0
Steps: 18 	 Acc:59.28395061728395 	 Loss:0
Steps: 19 	 Acc:59.28395061728395 	 Loss:0
Steps: 20 	 Acc:58.08641975308642 	 Loss:0
Steps: 21 	 Acc:58.08641975308642 	 Loss:0
Steps: 22 	 Acc:58.08641975308642 	 Loss:0
Steps: 23 	 Acc:58.08641975308642 	 Loss:0
Steps: 24 	 Acc:58.08641975308642 	 Loss:0
Steps: 25 	 Acc:58.08641975308642 	 Loss:0
Steps: 26 	 Acc:58.08641975308642 	 Loss:0
Steps: 27 	 Acc:58.08641975308642 	 Loss:0
Steps: 28 	 Acc:58.08641975308642 	 Loss:0
Steps: 29 	 Acc:58.08641975308642 	 Loss:0
Steps: 30 	 Acc:65.34567901234568 	 Loss:0
Steps: 31 	 Acc:65.34567901234568 	 Loss:0
Steps: 32 	 Acc:65.34567901234568 	 Loss:0
Steps: 33 	 Acc:65.34567901234568 	 Loss:0
Steps: 34 	 Acc:65.34567901234568 	 Loss:0
Steps: 35 	 Acc:65.34567901234568 	 Loss:0
Steps: 36 	 Acc:65.34567901234568 	 Loss:0
Steps: 37 	 Acc:65.34567901234568 	 Loss:0
Steps: 38 	 Acc:65.34567901234568 	 Loss:0
Steps: 39 	 Acc:65.34567901234568 	 Loss:0
Steps: 40 	 Acc:70.23456790123457 	 Loss:0
Steps: 41 	 Acc:70.23456790123457 	 Loss:0
Steps: 42 	 Acc:70.23456790123457 	 Loss:0
Steps: 43 	 Acc:70.23456790123457 	 Loss:0
Steps: 44 	 Acc:70.23456790123457 	 Loss:0
Steps: 45 	 Acc:70.23456790123457 	 Loss:0
Steps: 46 	 Acc:70.23456790123457 	 Loss:0
Steps: 47 	 Acc:70.23456790123457 	 Loss:0
Steps: 48 	 Acc:70.23456790123457 	 Loss:0
Steps: 49 	 Acc:70.23456790123457 	 Loss:0
Steps: 50 	 Acc:67.8395061728395 	 Loss:0
Steps: 51 	 Acc:67.8395061728395 	 Loss:0
Steps: 52 	 Acc:67.8395061728395 	 Loss:0
