
Running configs.
{'root_path': 'datasets', 'dataset': 'eurosat', 'shots': 16, 'backbone': 'RN50', 'n_prompt_tokens': 4, 'intrinsic_dim': 512, 'budget': 8000, 'bound': 5, 'alpha': 1, 'popsize': 40, 'parallel': False, 'print_every': 50, 'eval_every': 100, 'n': 1, 'batch_size': 256, 'std': 1, 'weight': 0.01, 'train_epoch': 20, 'subsample_classes': 'all', 'seed': 1}

Preparing dataset.
Reading split from /raid0-data/yifan/datasets/eurosat/split_zhou_EuroSAT.json
Creating a 16-shot dataset

Loading visual features and labels from test set.
Initial context: "a photo of a"
Number of context words (tokens): 4
Population Size: 40
Serial Evaluation.
Printing trainable parameters
Printing to verify trainable parameters
name input_param param.shape: torch.Size([40, 512])
Iterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]
    1     40 1.662860870361328e+00 1.0e+00 3.92e-01  4e-01  4e-01 0:07.9
/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Steps: 0 	 Acc:48.23456790123457 	 Loss:0
    2     80 1.637218832969666e+00 1.0e+00 3.86e-01  4e-01  4e-01 0:15.8
Steps: 1 	 Acc:50.34567901234568 	 Loss:0
    3    120 1.546686887741089e+00 1.0e+00 3.80e-01  4e-01  4e-01 0:23.4
Steps: 2 	 Acc:43.82716049382716 	 Loss:0
    4    160 1.579167008399963e+00 1.0e+00 3.75e-01  4e-01  4e-01 0:31.1
Steps: 3 	 Acc:47.30864197530864 	 Loss:0
    5    200 1.563799142837524e+00 1.0e+00 3.70e-01  4e-01  4e-01 0:38.8
Steps: 4 	 Acc:54.18518518518518 	 Loss:0
    6    240 1.519938468933105e+00 1.0e+00 3.65e-01  4e-01  4e-01 0:46.5
Steps: 5 	 Acc:56.02469135802469 	 Loss:0
    7    280 1.457567453384399e+00 1.0e+00 3.61e-01  4e-01  4e-01 0:54.3
Steps: 6 	 Acc:54.23456790123457 	 Loss:0
    8    320 1.477479934692383e+00 1.0e+00 3.58e-01  4e-01  4e-01 1:02.0
Steps: 7 	 Acc:55.80246913580247 	 Loss:0
Steps: 8 	 Acc:58.17283950617284 	 Loss:0
   10    400 1.404731512069702e+00 1.0e+00 3.51e-01  4e-01  4e-01 1:17.4
Steps: 9 	 Acc:58.46913580246913 	 Loss:0
Steps: 10 	 Acc:59.28395061728395 	 Loss:0
   12    480 1.398764014244080e+00 1.0e+00 3.46e-01  3e-01  3e-01 1:32.8
Steps: 11 	 Acc:62.851851851851855 	 Loss:0
Steps: 12 	 Acc:61.28395061728395 	 Loss:0
   14    560 1.338399648666382e+00 1.0e+00 3.42e-01  3e-01  3e-01 1:48.1
Steps: 13 	 Acc:60.34567901234568 	 Loss:0
Steps: 14 	 Acc:62.5679012345679 	 Loss:0
   16    640 1.299560785293579e+00 1.0e+00 3.38e-01  3e-01  3e-01 2:03.7
Steps: 15 	 Acc:61.135802469135804 	 Loss:0
Steps: 16 	 Acc:63.41975308641975 	 Loss:0
   18    720 1.298551797866821e+00 1.0e+00 3.35e-01  3e-01  3e-01 2:19.0
Steps: 17 	 Acc:59.641975308641975 	 Loss:0
Steps: 18 	 Acc:60.41975308641975 	 Loss:0
   20    800 1.260840654373169e+00 1.0e+00 3.32e-01  3e-01  3e-01 2:34.3
Steps: 19 	 Acc:59.864197530864196 	 Loss:0
Steps: 20 	 Acc:58.08641975308642 	 Loss:0
   22    880 1.275743722915649e+00 1.0e+00 3.30e-01  3e-01  3e-01 2:49.8
Steps: 21 	 Acc:59.08641975308642 	 Loss:0
Steps: 22 	 Acc:58.358024691358025 	 Loss:0
   24    960 1.254155397415161e+00 1.0e+00 3.27e-01  3e-01  3e-01 3:05.1
Steps: 23 	 Acc:60.34567901234568 	 Loss:0
Steps: 24 	 Acc:58.72839506172839 	 Loss:0
Steps: 25 	 Acc:59.46913580246913 	 Loss:0
   27   1080 1.219642043113708e+00 1.0e+00 3.25e-01  3e-01  3e-01 3:28.1
Steps: 26 	 Acc:60.32098765432099 	 Loss:0
Steps: 27 	 Acc:61.51851851851852 	 Loss:0
