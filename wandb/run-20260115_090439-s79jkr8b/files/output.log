
Running configs.
{'root_path': 'datasets', 'dataset': 'eurosat', 'shots': 16, 'backbone': 'RN50', 'n_prompt_tokens': 4, 'intrinsic_dim': 512, 'budget': 8000, 'bound': 5, 'alpha': 1, 'popsize': 40, 'parallel': False, 'print_every': 50, 'eval_every': 100, 'n': 1, 'batch_size': 256, 'std': 1, 'weight': 0.01, 'train_epoch': 20, 'subsample_classes': 'all', 'seed': 1}

Preparing dataset.
Reading split from /raid0-data/yifan/datasets/eurosat/split_zhou_EuroSAT.json
Creating a 16-shot dataset

Loading visual features and labels from test set.
Traceback (most recent call last):
  File "/home/yifanyang/tmp/mllm/neurips_code/main_sharpzo.py", line 622, in <module>
    main()
  File "/home/yifanyang/tmp/mllm/neurips_code/main_sharpzo.py", line 611, in main
    acc = run(args, cfg, dataset, clip_model, val_loader, test_loader, train_loader_F, run_name=wandb_run_name)
  File "/home/yifanyang/tmp/mllm/neurips_code/main_sharpzo.py", line 413, in run
    prompt_learner = PromptLearner(args, cfg, dataset.classnames, clip_model, val_loader, train_loader_F).cuda()
  File "/home/yifanyang/tmp/mllm/neurips_code/main_sharpzo.py", line 269, in __init__
    self.train_features, self.train_labels = pre_load_features(cfg, "train", clip_model, train_loader_F)
  File "/home/yifanyang/tmp/mllm/neurips_code/utils.py", line 42, in pre_load_features
    image_features = clip_model.encode_image(images)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 337, in encode_image
    return self.visual(image.type(self.dtype))
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 228, in forward
    x = self.transformer(x)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 199, in forward
    return self.resblocks(x)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 187, in forward
    x = x + self.mlp(self.ln_2(x))
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 164, in forward
    return x * torch.sigmoid(1.702 * x)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 370.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 48.19 MiB is free. Process 433950 has 1.93 GiB memory in use. Process 442488 has 3.74 GiB memory in use. Process 446020 has 3.74 GiB memory in use. Process 450579 has 3.74 GiB memory in use. Process 479101 has 3.74 GiB memory in use. Process 496316 has 1.02 GiB memory in use. Process 541105 has 3.74 GiB memory in use. Process 544347 has 3.74 GiB memory in use. Process 548776 has 3.74 GiB memory in use. Process 554980 has 3.74 GiB memory in use. Process 558667 has 3.74 GiB memory in use. Including non-PyTorch memory, this process has 2.78 GiB memory in use. Of the allocated memory 1.76 GiB is allocated by PyTorch, and 541.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/yifanyang/tmp/mllm/neurips_code/main_sharpzo.py", line 622, in <module>
    main()
  File "/home/yifanyang/tmp/mllm/neurips_code/main_sharpzo.py", line 611, in main
    acc = run(args, cfg, dataset, clip_model, val_loader, test_loader, train_loader_F, run_name=wandb_run_name)
  File "/home/yifanyang/tmp/mllm/neurips_code/main_sharpzo.py", line 413, in run
    prompt_learner = PromptLearner(args, cfg, dataset.classnames, clip_model, val_loader, train_loader_F).cuda()
  File "/home/yifanyang/tmp/mllm/neurips_code/main_sharpzo.py", line 269, in __init__
    self.train_features, self.train_labels = pre_load_features(cfg, "train", clip_model, train_loader_F)
  File "/home/yifanyang/tmp/mllm/neurips_code/utils.py", line 42, in pre_load_features
    image_features = clip_model.encode_image(images)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 337, in encode_image
    return self.visual(image.type(self.dtype))
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 228, in forward
    x = self.transformer(x)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 199, in forward
    return self.resblocks(x)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 187, in forward
    x = x + self.mlp(self.ln_2(x))
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/yifanyang/miniconda3/envs/vlm/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/yifanyang/tmp/mllm/neurips_code/clip/model.py", line 164, in forward
    return x * torch.sigmoid(1.702 * x)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 370.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 48.19 MiB is free. Process 433950 has 1.93 GiB memory in use. Process 442488 has 3.74 GiB memory in use. Process 446020 has 3.74 GiB memory in use. Process 450579 has 3.74 GiB memory in use. Process 479101 has 3.74 GiB memory in use. Process 496316 has 1.02 GiB memory in use. Process 541105 has 3.74 GiB memory in use. Process 544347 has 3.74 GiB memory in use. Process 548776 has 3.74 GiB memory in use. Process 554980 has 3.74 GiB memory in use. Process 558667 has 3.74 GiB memory in use. Including non-PyTorch memory, this process has 2.78 GiB memory in use. Of the allocated memory 1.76 GiB is allocated by PyTorch, and 541.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
